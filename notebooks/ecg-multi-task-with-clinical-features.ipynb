{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: ok\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\ecg\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Anaconda3\\envs\\ecg\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "c:\\Anaconda3\\envs\\ecg\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "c:\\Anaconda3\\envs\\ecg\\lib\\site-packages\\pkg_resources\\__init__.py:123: PkgResourcesDeprecationWarning: llow is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path as op\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, StochasticWeightAveraging\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from mtecg import MultiTaskClinicalCNNDataset, MultiTaskClinicalCNNModel\n",
    "from mtecg.utils import load_ecg_dataframe\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "seed_everything(SEED, workers=True)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvef_threshold = 50\n",
    "image_size= (384, 384)\n",
    "batch_size = 16\n",
    "num_epochs = 10\n",
    "\n",
    "configs = {\n",
    "    # ECG image.\n",
    "    \"in_channels\": 3,\n",
    "    \"learning_rate\": 5e-4,\n",
    "    \"use_timm\": True,\n",
    "    \"pretrained\": True,\n",
    "    \"backbone\": \"resnet34d\",\n",
    "    \"latent_dim\": 512,\n",
    "    \"scar_class\": 2,\n",
    "    \"lvef_class\": 2,\n",
    "    \"scar_lvef_loss_ratio\": [0.7, 0.3],\n",
    "    \"bias_head\": True,\n",
    "    # Clinical features.\n",
    "    \"embedding_size\" : 5,\n",
    "    \"rnn_type\": \"lstm\",\n",
    "    \"num_rnn_layers\" : 1,\n",
    "    \"rnn_output_size\" : 128,\n",
    "    \"num_categorical_features\" : 5,\n",
    "    \"num_numerical_features\" : 1,\n",
    "    # Specify the device.\n",
    "    \"device\": \"cuda\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_save_dir = f\"../trained_models/multi-task-clinical\"\n",
    "os.makedirs(parent_save_dir, exist_ok=True)\n",
    "\n",
    "run_suffix = f\"{image_size[0]}_LVEF{str(lvef_threshold)}_dim{configs['rnn_output_size']}\"\n",
    "run_name = f\"{configs['backbone']}_{run_suffix}\"\n",
    "\n",
    "os.makedirs(op.join(parent_save_dir, run_name), exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Prepare the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 13343\n",
      "Unique splits: ['old_train' 'old_valid' 'old_test' 'new_train' 'new_valid']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_num</th>\n",
       "      <th>train_80_percent</th>\n",
       "      <th>develop_10_percent</th>\n",
       "      <th>file_name</th>\n",
       "      <th>lvef</th>\n",
       "      <th>scar_cad</th>\n",
       "      <th>hcm</th>\n",
       "      <th>mri_date</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>...</th>\n",
       "      <th>dm</th>\n",
       "      <th>ht</th>\n",
       "      <th>mi</th>\n",
       "      <th>pci</th>\n",
       "      <th>cabg</th>\n",
       "      <th>ua</th>\n",
       "      <th>chest pain</th>\n",
       "      <th>dyspnea</th>\n",
       "      <th>path</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009_420521391</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2552-08-01 00:00:00</td>\n",
       "      <td>8</td>\n",
       "      <td>2009</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>../../ecg/ecg-cnn-local/siriraj_data/ECG_MRI_i...</td>\n",
       "      <td>old_train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009_472422791</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2552-08-01 00:00:00</td>\n",
       "      <td>8</td>\n",
       "      <td>2009</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>../../ecg/ecg-cnn-local/siriraj_data/ECG_MRI_i...</td>\n",
       "      <td>old_train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009_451191451</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2552-08-01 00:00:00</td>\n",
       "      <td>8</td>\n",
       "      <td>2009</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>../../ecg/ecg-cnn-local/siriraj_data/ECG_MRI_i...</td>\n",
       "      <td>old_train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009_512029431</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2552-08-01 00:00:00</td>\n",
       "      <td>8</td>\n",
       "      <td>2009</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>../../ecg/ecg-cnn-local/siriraj_data/ECG_MRI_i...</td>\n",
       "      <td>old_train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009_461543281</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2552-08-04 00:00:00</td>\n",
       "      <td>8</td>\n",
       "      <td>2009</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>../../ecg/ecg-cnn-local/siriraj_data/ECG_MRI_i...</td>\n",
       "      <td>old_train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   run_num  train_80_percent  develop_10_percent       file_name  lvef  \\\n",
       "0        1               1.0                 NaN  2009_420521391     0   \n",
       "1        2               1.0                 NaN  2009_472422791     0   \n",
       "2        3               1.0                 NaN  2009_451191451     0   \n",
       "3        4               1.0                 NaN  2009_512029431     1   \n",
       "4        5               1.0                 NaN  2009_461543281     1   \n",
       "\n",
       "   scar_cad  hcm             mri_date  month  year  ...  dm  ht  mi  pci  \\\n",
       "0         0    0  2552-08-01 00:00:00      8  2009  ...   0   1   0    0   \n",
       "1         0    0  2552-08-01 00:00:00      8  2009  ...   0   1   0    0   \n",
       "2         0    0  2552-08-01 00:00:00      8  2009  ...   0   1   0    0   \n",
       "3         1    0  2552-08-01 00:00:00      8  2009  ...   1   0   1    1   \n",
       "4         1    0  2552-08-04 00:00:00      8  2009  ...   0   1   0    0   \n",
       "\n",
       "   cabg  ua  chest pain  dyspnea  \\\n",
       "0     0   0           1        0   \n",
       "1     0   0           1        0   \n",
       "2     0   0           1        1   \n",
       "3     0   0           0        1   \n",
       "4     0   0           1        1   \n",
       "\n",
       "                                                path      split  \n",
       "0  ../../ecg/ecg-cnn-local/siriraj_data/ECG_MRI_i...  old_train  \n",
       "1  ../../ecg/ecg-cnn-local/siriraj_data/ECG_MRI_i...  old_train  \n",
       "2  ../../ecg/ecg-cnn-local/siriraj_data/ECG_MRI_i...  old_train  \n",
       "3  ../../ecg/ecg-cnn-local/siriraj_data/ECG_MRI_i...  old_train  \n",
       "4  ../../ecg/ecg-cnn-local/siriraj_data/ECG_MRI_i...  old_train  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dir = \"../../ecg/ecg-cnn-local/siriraj_data/ECG_MRI_images_new/\"\n",
    "csv_path = \"../../ECG_EF_Clin_train_dev_new.csv\"\n",
    "\n",
    "df = load_ecg_dataframe(csv_path, image_dir, drop_impute=False, do_split=True)\n",
    "print(f\"Number of images: {len(df)}\")\n",
    "print(f\"Unique splits: {df['split'].unique()}\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set [\"dm\", \"ht\", \"smoke\", \"dlp\"] to np.nan if \"impute\" is True.\n",
    "df.loc[df[\"impute\"] == True, [\"dm\", \"ht\", \"smoke\", \"dlp\"]] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9393, 28), (2905, 28))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine old train and new train.\n",
    "train_df = df[df.split.isin([\"old_train\", \"new_train\"])].reset_index()\n",
    "# Combine old valid and new valid.\n",
    "valid_df = df[df.split.isin([\"old_valid\", \"new_valid\"])].reset_index()\n",
    "\n",
    "train_df.shape, valid_df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../trained_models/multi-task-clinical\\\\resnet34d_384_LVEF50_dim128\\\\imputer.joblib']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import joblib\n",
    "\n",
    "imputer = IterativeImputer(\n",
    "    missing_values=np.nan,\n",
    "    max_iter=10,\n",
    "    sample_posterior=True,\n",
    "    random_state=42\n",
    "    )\n",
    "\n",
    "clinical_feature_columns = [\"age\", \"female_gender\", \"dm\", \"ht\", \"smoke\", \"dlp\"]\n",
    "\n",
    "# Fit the imputer on the train set.\n",
    "imputer.fit(train_df[clinical_feature_columns])\n",
    "\n",
    "# Save the imputer.\n",
    "imputer_path = op.join(parent_save_dir, run_name, \"imputer.joblib\")\n",
    "joblib.dump(imputer, imputer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values in the train set.\n",
    "train_df[clinical_feature_columns] = imputer.transform(train_df[clinical_feature_columns])\n",
    "\n",
    "# Impute missing values in the valid set.\n",
    "valid_df[clinical_feature_columns] = imputer.transform(valid_df[clinical_feature_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import pandas as pd\n",
    "\n",
    "def find_best_thresholds(\n",
    "    dataframe: pd.DataFrame,\n",
    "    imputed_column_names: List[str] = [\"dm\", \"ht\", \"smoke\", \"dlp\"],\n",
    "    ):\n",
    "    original_df = dataframe[dataframe[\"impute\"] == False].reset_index()[imputed_column_names]\n",
    "    impute_df = dataframe[dataframe[\"impute\"] == True].reset_index()[imputed_column_names]\n",
    "\n",
    "    # Try out different threshold for categorizing the imputed values into 0 or 1 so that the distribution of the imputed values is similar to the original values.\n",
    "    # Store the result that has the smallest difference between the prevalence of the imputed values and the original values.\n",
    "    # The threshold that gives the smallest difference is the threshold that gives the best imputation.\n",
    "    # The threshold of each column is stored in a dictionary.\n",
    "    best_threshold_dict = {}\n",
    "    for imputed_column_name in imputed_column_names:\n",
    "        \n",
    "        for threshold in np.arange(0.1, 0.9, 0.01):\n",
    "            imputed_df = pd.DataFrame(\n",
    "                impute_df.values > threshold,\n",
    "                columns=imputed_column_names,\n",
    "                index=impute_df.index,\n",
    "                )\n",
    "            imputed_df = imputed_df.astype(int)\n",
    "            diff = abs(original_df[imputed_column_name].sum() / len(original_df) - imputed_df[imputed_column_name].sum() / len(imputed_df))\n",
    "            if imputed_column_name not in best_threshold_dict:\n",
    "                best_threshold_dict[imputed_column_name] = [round(threshold, 2), diff]\n",
    "            else:\n",
    "                if diff < best_threshold_dict[imputed_column_name][1]:\n",
    "                    best_threshold_dict[imputed_column_name] = [round(threshold, 2), diff]\n",
    "    return best_threshold_dict\n",
    "\n",
    "\n",
    "def apply_thresholds(\n",
    "    dataframe: pd.DataFrame,\n",
    "    best_threshold_dict: dict,\n",
    "    imputed_column_names: List[str] = [\"dm\", \"ht\", \"smoke\", \"dlp\"],\n",
    "    ):\n",
    "    impute_df = dataframe[dataframe[\"impute\"] == True].reset_index()[imputed_column_names]\n",
    "    for imputed_column_name in imputed_column_names:\n",
    "        threshold = best_threshold_dict[imputed_column_name][0]\n",
    "        imputed_df = pd.DataFrame(\n",
    "            impute_df.values > threshold,\n",
    "            columns=imputed_column_names,\n",
    "            index=impute_df.index,\n",
    "            )\n",
    "        imputed_df = imputed_df.astype(int)\n",
    "        dataframe.loc[dataframe[\"impute\"] == True, imputed_column_name] = imputed_df[imputed_column_name].values\n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best thresholds for imputing missing values from the train set.\n",
    "best_threshold_dict = find_best_thresholds(train_df)\n",
    "\n",
    "joblib.dump(best_threshold_dict, op.join(parent_save_dir, run_name, \"imputer_threshold_dict.joblib\"))\n",
    "\n",
    "# Apply the best thresholds to the train set and the valid set.\n",
    "train_df = apply_thresholds(train_df, best_threshold_dict)\n",
    "valid_df = apply_thresholds(valid_df, best_threshold_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "    A.Resize(*image_size),\n",
    "    A.Blur(blur_limit=3, p=0.2),\n",
    "    A.RandomBrightnessContrast(),\n",
    "    A.MotionBlur(p=0.2),\n",
    "    A.Normalize(),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "valid_transform = A.Compose([\n",
    "    A.Resize(*image_size),\n",
    "    A.Normalize(),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "\n",
    "train_ds = MultiTaskClinicalCNNDataset(train_df, train_transform, lvef_threshold=lvef_threshold)\n",
    "valid_ds = MultiTaskClinicalCNNDataset(valid_df, valid_transform, lvef_threshold=lvef_threshold)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=batch_size, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiTaskClinicalCNNModel(**configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "project_name = f\"ecg-multi-task-with-clinical-features\"\n",
    "\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"ecg-multi-task-with-clinical-features.ipynb\"\n",
    "run = wandb.init(project = project_name, save_code = True)\n",
    "run.log_code(\".\", include_fn = lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\"))\n",
    "run.config.update({\"batch_size\": batch_size,})\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filename = configs[\"backbone\"] + \"{val_acc:.2f}\",\n",
    "    save_top_k = 1,\n",
    "    verbose = True,\n",
    "    monitor = \"val_loss\",\n",
    "    mode = \"min\",\n",
    ")\n",
    "\n",
    "logger = WandbLogger(\n",
    "    project = project_name,\n",
    "    name = configs[\"backbone\"],\n",
    "    # log_model = \"all\", # set to True to log at the end\n",
    ")\n",
    "\n",
    "logger.watch(\n",
    "    model, \n",
    "    # log_freq=300, # uncomment to log gradients\n",
    "    log_graph = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type             | Params\n",
      "--------------------------------------------------------\n",
      "0 | accuracy           | Accuracy         | 0     \n",
      "1 | scar_loss_fn       | CrossEntropyLoss | 0     \n",
      "2 | lvef_loss_fn       | CrossEntropyLoss | 0     \n",
      "3 | model              | ResNet           | 21.6 M\n",
      "4 | scar_head          | Linear           | 1.3 K \n",
      "5 | lvef_head          | Linear           | 1.3 K \n",
      "6 | embedding_layer    | Embedding        | 10    \n",
      "7 | clinical_rnn_layer | LSTM             | 69.6 K\n",
      "--------------------------------------------------------\n",
      "21.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.6 M    Total params\n",
      "86.553    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4ad8e7341a7442da00095f91964a84b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\namea\\AppData\\Roaming\\Python\\Python38\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\namea\\AppData\\Roaming\\Python\\Python38\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "982db288d1834cea99969e5f945a1307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9dd6342a28b40d2bc6f096709473662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 588: 'val_loss' reached 0.32442 (best 0.32442), saving model to '.\\\\ecg-multi-task-with-clinical-features\\\\1yg3zjgs\\\\checkpoints\\\\resnet34dval_acc=0.86.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbb1bdc712774ee1af42b7c480aa6288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 1176: 'val_loss' reached 0.29559 (best 0.29559), saving model to '.\\\\ecg-multi-task-with-clinical-features\\\\1yg3zjgs\\\\checkpoints\\\\resnet34dval_acc=0.87.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4a40d7e37744b198a048d3a6311fb3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 1764: 'val_loss' reached 0.27795 (best 0.27795), saving model to '.\\\\ecg-multi-task-with-clinical-features\\\\1yg3zjgs\\\\checkpoints\\\\resnet34dval_acc=0.88.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdb70d14950c42a49bfea1365a64f768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 2352: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b6625159ad84d88aca629bce1142927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 2940: 'val_loss' reached 0.27077 (best 0.27077), saving model to '.\\\\ecg-multi-task-with-clinical-features\\\\1yg3zjgs\\\\checkpoints\\\\resnet34dval_acc=0.88.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebece5e3365a4dcfb935f86deb90268e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 3528: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07ab13eacd524373ae6df6ed7c406e69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 4116: 'val_loss' was not in top 1\n",
      "Swapping scheduler `ReduceLROnPlateau` for `SWALR`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9da740e4d934a7e8289807e4dfe04a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 4704: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c32fffd9bc5c4c01930c1f613cf90e31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 5292: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18e3b5474a6e4386b63932db6649e2af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 5880: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76892b2299434f40854c8380d8f0c4fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 6468: 'val_loss' was not in top 1\n",
      "`Trainer.fit` stopped: `max_epochs=11` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    logger = logger,\n",
    "    max_epochs = num_epochs,\n",
    "    callbacks = [checkpoint_callback, StochasticWeightAveraging(1e-3)],\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    model,\n",
    "    train_dataloaders = train_loader,\n",
    "    val_dataloaders = valid_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint(op.join(parent_save_dir, run_name, \"model.ckpt\"))\n",
    "model.save_configs(op.join(parent_save_dir, run_name))\n",
    "\n",
    "A.save(train_transform, op.join(parent_save_dir, run_name, \"train_transform.json\"))\n",
    "A.save(valid_transform, op.join(parent_save_dir, run_name, \"transform.json\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "109b74d06ceef90c267188b655f808679842e5df9d924ed4ca45afc3047e2ff5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
