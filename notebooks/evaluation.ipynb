{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Preparations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\ecg\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Anaconda3\\envs\\ecg\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "c:\\Anaconda3\\envs\\ecg\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "c:\\Anaconda3\\envs\\ecg\\lib\\site-packages\\pkg_resources\\__init__.py:123: PkgResourcesDeprecationWarning: llow is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path as op\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from mtecg.classifier import ECGClassifier\n",
    "from mtecg.evaluation import evaluate_from_dataframe\n",
    "from mtecg.utils import load_ecg_dataframe\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "round_probabilities = False\n",
    "\n",
    "singletask_scar_model_path = \"../trained_models/single-task-scar/resnet34d_384_LVEF50\"\n",
    "single_task_scar_clinical_model_path = \"../trained_models/single-task-scar-clinical/resnet34d_384_LVEF50_birnn_dim512\"\n",
    "singletask_lvef_model_path = \"../trained_models/single-task-lvef/resnet34d_384_LVEF50\"\n",
    "singletask_lvef_clinical_model_path = \"../trained_models/single-task-lvef-clinical/resnet34d_384_LVEF50_birnn_dim512\"\n",
    "\n",
    "multitask_old_format_model_path = \"../trained_models/multi-task-old-format/resnet34d_384_LVEF50\"\n",
    "multitask_transferred_model_path = \"../trained_models/multi-task-transferred/resnet34d_384_LVEF50\"\n",
    "multitask_model_path = \"../trained_models/multi-task/resnet34d_384_LVEF50\"\n",
    "multitask_clinical_model_path = \"../trained_models/multi-task-clinical/resnet34d_384_LVEF50_birnn_dim512/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "multitask_old_format_classifier = ECGClassifier(\n",
    "    multitask_old_format_model_path, model_class=\"multi-task\", device=device, round_probabilities=round_probabilities\n",
    ")\n",
    "multitask_transferred_classifier = ECGClassifier(\n",
    "    multitask_transferred_model_path, model_class=\"multi-task\", device=device, round_probabilities=round_probabilities\n",
    ")\n",
    "singletask_scar_classifier = ECGClassifier(\n",
    "    singletask_scar_model_path,\n",
    "    model_class=\"single-task\",\n",
    "    device=device,\n",
    "    task=\"scar\",\n",
    "    round_probabilities=round_probabilities,\n",
    ")\n",
    "singletask_scar_clinical_classifier = ECGClassifier(\n",
    "    singletask_scar_clinical_model_path,\n",
    "    model_class=\"single-task-clinical\",\n",
    "    device=device,\n",
    "    task=\"scar\",\n",
    "    round_probabilities=round_probabilities,\n",
    ")\n",
    "singletask_lvef_classifier = ECGClassifier(\n",
    "    singletask_lvef_model_path,\n",
    "    model_class=\"single-task\",\n",
    "    device=device,\n",
    "    task=\"lvef\",\n",
    "    round_probabilities=round_probabilities,\n",
    ")\n",
    "singletask_lvef_clinical_classifier = ECGClassifier(\n",
    "    singletask_lvef_clinical_model_path,\n",
    "    model_class=\"single-task-clinical\",\n",
    "    device=device,\n",
    "    task=\"lvef\",\n",
    "    round_probabilities=round_probabilities,\n",
    ")\n",
    "multitask_classifier = ECGClassifier(\n",
    "    multitask_model_path, model_class=\"multi-task\", device=device, round_probabilities=round_probabilities\n",
    ")\n",
    "multitask_clinical_classifier = ECGClassifier(\n",
    "    multitask_clinical_model_path,\n",
    "    model_class=\"multi-task-clinical\",\n",
    "    device=device,\n",
    "    round_probabilities=round_probabilities,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old_csv_path = \"../datasets/old_test_with_qrs_duration.csv\"\n",
    "# new_csv_path = \"../datasets/new_test_with_qrs_duration.csv\"\n",
    "# control_csv_path = \"../datasets/siriraj_data/ECG_Normal/ECG_normal_n2097_220906_modified.csv\"\n",
    "# control_image_dir = \"../datasets/siriraj_data/ECG_normal_images\"\n",
    "\n",
    "old_csv_path = \"../datasets/all_ECG_cleared_duplicate_may23_final.csv\"\n",
    "old_image_dir = \"../datasets/siriraj_data/ECG_MRI_images_new/\"\n",
    "new_csv_path = \"../datasets/all_ECG_cleared_duplicate_may23_final.csv\"\n",
    "new_image_dir = \"../datasets/siriraj_data/ECG_MRI_test_images_new/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old test set.\n",
    "old_test_df = load_ecg_dataframe(\n",
    "    old_csv_path, old_image_dir, imputer_dir=multitask_clinical_model_path, do_split=True\n",
    ")\n",
    "old_test_df = old_test_df[old_test_df[\"split\"] == \"old_test\"].reset_index(drop=True)\n",
    "old_test_df = load_ecg_dataframe(\n",
    "    old_csv_path, old_image_dir, imputer_dir=multitask_clinical_model_path, do_split=False\n",
    ")\n",
    "\n",
    "# Old test set with lvef_threshold= 40.\n",
    "sensitivity_old_test_df = load_ecg_dataframe(\n",
    "    old_csv_path,\n",
    "    old_image_dir,\n",
    "    imputer_dir=multitask_clinical_model_path,\n",
    "    do_split=True,\n",
    "    lvef_threshold=40,\n",
    ")\n",
    "sensitivity_old_test_df = sensitivity_old_test_df[sensitivity_old_test_df[\"split\"] == \"old_test\"].reset_index(drop=True)\n",
    "\n",
    "# New test set. No need to impute.\n",
    "new_test_df = load_ecg_dataframe(\n",
    "    new_csv_path,\n",
    "    new_image_dir,\n",
    "    # imputer_dir=multitask_clinical_model_path,\n",
    "    do_split=False,\n",
    ")\n",
    "\n",
    "# New test set with lvef_threshold= 40. No need to impute.\n",
    "sensitivity_new_test_df = load_ecg_dataframe(\n",
    "    new_csv_path,\n",
    "    new_image_dir,\n",
    "    # imputer_dir=multitask_clinical_model_path,\n",
    "    do_split=False,\n",
    "    lvef_threshold=40,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For convenience.\n",
    "\n",
    "EVAL_DATA_MAP = {\n",
    "    \"old-test\": {\"data\": old_test_df, \"save_suffix\": \"old_test\",},\n",
    "    \"old-test-sensitivity\": {\"data\": sensitivity_old_test_df, \"save_suffix\": \"old_test_sensitivity\",},\n",
    "    \"new-test\": {\"data\": new_test_df, \"save_suffix\": \"new_test\",},\n",
    "    \"new-test-sensitivity\": {\"data\": sensitivity_new_test_df, \"save_suffix\": \"new_test_sensitivity\",},\n",
    "    # \"control-test\": {\"data\": control_test_df, \"save_suffix\": \"control_test\",},\n",
    "}\n",
    "\n",
    "TEST_SET_SAVE_SUFFIX_LIST = [param_dict[\"save_suffix\"] for param_dict in EVAL_DATA_MAP.values()]\n",
    "\n",
    "\n",
    "def evaluate_and_save(\n",
    "    classifier: ECGClassifier,\n",
    "    save_dir: str,\n",
    "    average: str = \"weighted\",\n",
    "    prediction_csv_name_pattern: str = \"prediction_{save_suffix}.csv\",\n",
    "    metric_csv_name_pattern: str = \"metrics_{save_suffix}.csv\",\n",
    "):\n",
    "    for test_set_name, param_dict in tqdm(EVAL_DATA_MAP.items()):\n",
    "        dataframe, save_suffix = param_dict[\"data\"], param_dict[\"save_suffix\"]\n",
    "        if \"control\" in test_set_name:\n",
    "            result_dataframe, metric_dataframe = evaluate_from_dataframe(\n",
    "                dataframe,\n",
    "                classifier,\n",
    "                is_control_population=True,\n",
    "                average=average,\n",
    "                )\n",
    "        else:\n",
    "            result_dataframe, metric_dataframe = evaluate_from_dataframe(dataframe, classifier)\n",
    "\n",
    "        result_save_path = op.join(save_dir, prediction_csv_name_pattern.format(save_suffix=save_suffix))\n",
    "        metric_save_path = op.join(save_dir, metric_csv_name_pattern.format(save_suffix=save_suffix))\n",
    "\n",
    "        result_dataframe.to_csv(result_save_path, index=False)\n",
    "        metric_dataframe.to_csv(metric_save_path, index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Evaluation**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Baseline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "def get_baseline_metrics(\n",
    "    save_dir: str = None,\n",
    "    label_column_name: str = \"scar_cad\",\n",
    "    metric_csv_name_pattern: str = \"{save_suffix}.csv\",\n",
    "    average: str = \"weighted\",\n",
    "    ):\n",
    "    baseline_metric_dict = {}\n",
    "    for test_set_name, param_dict in EVAL_DATA_MAP.items():\n",
    "        if \"control\" in test_set_name:\n",
    "            continue\n",
    "        dataframe = param_dict[\"data\"]\n",
    "        baseline_predictions = np.zeros(len(dataframe))\n",
    "\n",
    "        # get specificity from confusion matrix\n",
    "        tn, fp, fn, tp = confusion_matrix(dataframe[label_column_name], baseline_predictions).ravel()\n",
    "        specificity = tn / (tn+fp)\n",
    "        fpr = fp / (fp+tn)\n",
    "\n",
    "        baseline_metric_dict[test_set_name] = {\n",
    "            \"Accuracy\": accuracy_score(dataframe[label_column_name], baseline_predictions),\n",
    "            \"Sensitivity\": None,\n",
    "            \"Specificity\": specificity,\n",
    "            \"F1\": f1_score(dataframe[label_column_name], baseline_predictions, average=average),\n",
    "            \"AUC\": None,\n",
    "            \"FPR\": fpr,\n",
    "            \"FNR\": None,\n",
    "        }\n",
    "\n",
    "    if save_dir:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        for test_set_name, param_dict in EVAL_DATA_MAP.items():\n",
    "            if \"control\" in test_set_name:\n",
    "                continue\n",
    "            save_suffix = param_dict[\"save_suffix\"]\n",
    "            metric_save_path = op.join(save_dir, metric_csv_name_pattern.format(save_suffix=save_suffix))\n",
    "            pd.DataFrame(baseline_metric_dict[test_set_name], index=[0]).T.to_csv(metric_save_path, index=True)\n",
    "\n",
    "    return baseline_metric_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scar_baseline_metric_dict = get_baseline_metrics(\n",
    "    save_dir=op.join(\"../resources/statistics/scar_baseline_metrics\"),\n",
    "    label_column_name=\"scar_cad\",\n",
    ")\n",
    "\n",
    "lvef_baseline_metric_dict = get_baseline_metrics(\n",
    "    save_dir=op.join(\"../resources/statistics/lvef_baseline_metrics\"),\n",
    "    label_column_name=\"lvef\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **single-task**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_and_save(singletask_scar_classifier, save_dir=singletask_scar_model_path)\n",
    "evaluate_and_save(singletask_lvef_classifier, save_dir=singletask_lvef_model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **single-task-clinical**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_and_save(singletask_scar_clinical_classifier, save_dir=singletask_scar_clinical_model_path)\n",
    "evaluate_and_save(singletask_lvef_clinical_classifier, save_dir=singletask_lvef_clinical_model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **multi-task**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_and_save(multitask_classifier, save_dir=multitask_model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**multi-task-old-format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_and_save(multitask_old_format_classifier, save_dir=multitask_old_format_model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**multi-task-transferred**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_and_save(multitask_transferred_classifier, save_dir=multitask_transferred_model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **multi-task-clinical**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwrite the EVAL_DATA_MAP to only evaluate the test sets with available clinical data.\n",
    "EVAL_DATA_MAP = {\n",
    "    \"old-test\": {\"data\": old_test_df, \"save_suffix\": \"old_test\",},\n",
    "    \"old-test-sensitivity\": {\"data\": sensitivity_old_test_df, \"save_suffix\": \"old_test_sensitivity\",},\n",
    "    \"new-test\": {\"data\": new_test_df, \"save_suffix\": \"new_test\",},\n",
    "    \"new-test-sensitivity\": {\"data\": sensitivity_new_test_df, \"save_suffix\": \"new_test_sensitivity\",},\n",
    "}\n",
    "\n",
    "evaluate_and_save(multitask_clinical_classifier, save_dir=multitask_clinical_model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[scar] Prevalence-specific Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def sample(dataframe, num_samples: int = 500, prevalence: float = 7.9):\n",
    "    \"\"\"\n",
    "    Sample a dataframe to a prevalence of 7.9% (the prevalence of scar in the dataset).\n",
    "    \"\"\"\n",
    "    positive_dataframe = dataframe[dataframe.scar_label == 1].reset_index(drop=True).copy()\n",
    "    negative_dataframe = dataframe[dataframe.scar_label == 0].reset_index(drop=True).copy()\n",
    "    \n",
    "    num_positive_samples = int(num_samples * prevalence / 100)\n",
    "    num_negative_samples = num_samples - num_positive_samples\n",
    "    \n",
    "    random_state = random.randint(0, 1000)\n",
    "    positive_sample_dataframe = positive_dataframe.sample(n=num_positive_samples, random_state = random_state).reset_index(drop=True)\n",
    "    negative_sample_dataframe = negative_dataframe.sample(n=num_negative_samples, random_state = random_state).reset_index(drop=True)\n",
    "    sampled_dataframe = pd.concat([positive_sample_dataframe, negative_sample_dataframe]).reset_index(drop=True)\n",
    "    return sampled_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "from mtecg.evaluation import calculate_metrics\n",
    "\n",
    "num_sample_per_iteration = 500\n",
    "# num_iteration_list = [20, 50, 200]\n",
    "num_iteration_list = [1000]\n",
    "\n",
    "\n",
    "multitask_prediction_df = pd.read_csv(op.join(multitask_model_path, \"prediction_new_test.csv\"))\n",
    "\n",
    "for num_iteration in tqdm(num_iteration_list):\n",
    "    scar_auc_list = []\n",
    "    scar_f1_list = []\n",
    "    for i in tqdm(range(num_iteration)):\n",
    "        sampled_test_df = sample(multitask_prediction_df, num_samples = num_sample_per_iteration)\n",
    "        metric_df = calculate_metrics(sampled_test_df, tasks=[\"scar\"])\n",
    "        # _, metric_df = evaluate_from_dataframe(sampled_test_df, multitask_classifier)\n",
    "        # _, metric_df = evaluate_from_dataframe(sampled_clinical_test_df, multitask_clinical_classifier)\n",
    "        scar_auc_list.append(metric_df.T[\"AUC\"][0])\n",
    "        scar_f1_list.append(metric_df.T[\"F1\"][0])\n",
    "\n",
    "    # Create 95% confidence interval for population mean scar auc.\n",
    "    auc_confidence_interval_tuple = st.t.interval(\n",
    "        alpha=0.95,\n",
    "        df=len(scar_auc_list)-1,\n",
    "        loc=np.mean(scar_auc_list),\n",
    "        scale=st.sem(scar_auc_list)\n",
    "        )\n",
    "    \n",
    "    f1_confidence_interval_tuple = st.t.interval(\n",
    "        alpha=0.95,\n",
    "        df=len(scar_f1_list)-1,\n",
    "        loc=np.mean(scar_f1_list),\n",
    "        scale=st.sem(scar_f1_list)\n",
    "        )\n",
    "\n",
    "    auc_summary_df = pd.DataFrame(\n",
    "        { \n",
    "            \"mean\": [np.mean(scar_auc_list)],\n",
    "            \"std\": [np.std(scar_auc_list)],\n",
    "            \"lower_bound_ci\": [auc_confidence_interval_tuple[0]],\n",
    "            \"upper_bound_ci\": [auc_confidence_interval_tuple[1]],\n",
    "            }\n",
    "    )\\\n",
    "        .round(4)\n",
    "\n",
    "    f1_summary_df = pd.DataFrame(\n",
    "        { \n",
    "            \"mean\": [np.mean(scar_f1_list)],\n",
    "            \"std\": [np.std(scar_f1_list)],\n",
    "            \"lower_bound_ci\": [f1_confidence_interval_tuple[0]],\n",
    "            \"upper_bound_ci\": [f1_confidence_interval_tuple[1]],\n",
    "            }\n",
    "    )\\\n",
    "        .round(4)\n",
    "\n",
    "    auc_summary_df.to_csv(op.join(multitask_model_path, f\"prevalence_specific_auc_{num_sample_per_iteration}_{num_iteration}.csv\"), index=False)\n",
    "    f1_summary_df.to_csv(op.join(multitask_model_path, f\"prevalence_specific_f1_{num_sample_per_iteration}_{num_iteration}.csv\"), index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **XGB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to get XGBoost predictions.\n",
    "from mtecg.evaluation import calculate_metrics_per_task\n",
    "import mtecg.constants as constants\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "def evaluate_xgb_from_dataframe(\n",
    "    dataframe: pd.DataFrame,\n",
    "    model,\n",
    "    feature_column_names: List[str],\n",
    "    label_column_name: str = \"scar_cad\",\n",
    "    is_control_population=False,\n",
    "    task=\"scar\",\n",
    "    average=\"weighted\",\n",
    "    ):\n",
    "    x = dataframe[feature_column_names]\n",
    "    predicted_probability_array = model.predict_proba(x)[:, 1]\n",
    "    prediction_array = model.predict(x)\n",
    "\n",
    "    prediction_dataframe = pd.DataFrame(\n",
    "        {\n",
    "            f\"{task}_label\": dataframe[label_column_name].values,\n",
    "            f\"{task}_prediction\": prediction_array,\n",
    "            f\"{task}_probability\": predicted_probability_array,\n",
    "            \"filename\": dataframe[\"file_name\"].values,\n",
    "        }\n",
    "    )\n",
    "    metrics_dataframe = calculate_metrics_per_task(\n",
    "        prediction_dataframe,\n",
    "        task,\n",
    "        is_control_population=is_control_population,\n",
    "        average=average,\n",
    "        )\n",
    "    return prediction_dataframe, metrics_dataframe\n",
    "\n",
    "def evaluate_xgb_and_save(\n",
    "    xgb_model,\n",
    "    save_dir: str,\n",
    "    task: str = \"scar\",\n",
    "    average: str = \"weighted\",\n",
    "    feature_column_names: List[str] = constants.numerical_feature_column_names + constants.categorical_feature_column_names,\n",
    "    label_column_name: str = \"scar_cad\",\n",
    "    prediction_csv_name_pattern: str = \"prediction_{save_suffix}.csv\",\n",
    "    metric_csv_name_pattern: str = \"metrics_{save_suffix}.csv\",\n",
    "):\n",
    "    for test_set_name, param_dict in tqdm(EVAL_DATA_MAP.items()):\n",
    "        dataframe, save_suffix = param_dict[\"data\"], param_dict[\"save_suffix\"]\n",
    "        if \"control\" in test_set_name:\n",
    "            result_dataframe, metric_dataframe = evaluate_xgb_from_dataframe(\n",
    "                dataframe,\n",
    "                xgb_model,\n",
    "                task=task,\n",
    "                feature_column_names=feature_column_names,\n",
    "                is_control_population=True,\n",
    "                average=average,\n",
    "                )\n",
    "        else:\n",
    "            result_dataframe, metric_dataframe = evaluate_xgb_from_dataframe(\n",
    "                dataframe,\n",
    "                xgb_model,\n",
    "                task=task,\n",
    "                feature_column_names=feature_column_names,\n",
    "                average=average,\n",
    "                )\n",
    "\n",
    "        result_save_path = op.join(save_dir, prediction_csv_name_pattern.format(save_suffix=save_suffix))\n",
    "        metric_save_path = op.join(save_dir, metric_csv_name_pattern.format(save_suffix=save_suffix))\n",
    "\n",
    "        result_dataframe.to_csv(result_save_path, index=False)\n",
    "        metric_dataframe.to_csv(metric_save_path, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "xgb_model_dir = \"../scripts/mtecg/xgb\"\n",
    "scar_xgb_model_dir = op.join(xgb_model_dir, \"scar_model\")\n",
    "lvef_xgb_model_dir = op.join(xgb_model_dir, \"lvef_model\")\n",
    "\n",
    "scar_xgb_classifier = joblib.load(op.join(scar_xgb_model_dir, \"model.joblib\"))\n",
    "lvef_xgb_classifier = joblib.load(op.join(lvef_xgb_model_dir, \"model.joblib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d499e3c2770b4e41b9a4e7960a588acf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58a4dcf6572b4d4295ed877be89e5759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_xgb_and_save(\n",
    "    scar_xgb_classifier,\n",
    "    task=\"scar\",\n",
    "    label_column_name=\"scar_cad\",\n",
    "    save_dir=scar_xgb_model_dir,\n",
    "    )\n",
    "\n",
    "evaluate_xgb_and_save(\n",
    "    lvef_xgb_classifier,\n",
    "    task=\"lvef\",\n",
    "    label_column_name=\"lvef\",\n",
    "    save_dir=lvef_xgb_model_dir,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Save Prediction Probabilities on Each Test Set as a single file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Dict, List\n",
    "\n",
    "# A function to read the predictions from the save csv file in each model folder.\n",
    "# The probability columns of each task are then concatenated into a single dataframe.\n",
    "# The probability columns are in the format f\"{task}_probability\".\n",
    "\n",
    "def get_probabilities(\n",
    "    model_name_to_dir_map: Dict[str, str],\n",
    "    test_set_suffix_list: List[str],\n",
    "    probability_column_name_pattern: str = \"{task}_probability\",\n",
    "    prediction_csv_name_pattern: str = \"prediction_{test_set_suffix}.csv\",\n",
    "    task: str = \"scar\",\n",
    ") -> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Get the probabilities of the given task from the predictions of the models.\n",
    "    \"\"\"\n",
    "    probability_column_name = probability_column_name_pattern.format(task=task)\n",
    "\n",
    "    test_set_to_probability_dataframe_dict = {}\n",
    "    for test_set_suffix in test_set_suffix_list:\n",
    "        if \"control\" in test_set_suffix:\n",
    "            continue\n",
    "        model_name_to_probabilities_dict = {}\n",
    "        for model_name, model_dir in model_name_to_dir_map.items():\n",
    "            filename = prediction_csv_name_pattern.format(test_set_suffix=test_set_suffix)\n",
    "            prediction_path = op.join(model_dir, filename)\n",
    "            prediction_dataframe = pd.read_csv(prediction_path)\n",
    "            if \"true_label\" not in model_name_to_probabilities_dict.keys():\n",
    "                model_name_to_probabilities_dict[\"true_label\"] = prediction_dataframe[f\"{task}_label\"]\n",
    "            model_name_to_probabilities_dict[model_name] = prediction_dataframe[probability_column_name]\n",
    "        probability_dataframe = pd.DataFrame(model_name_to_probabilities_dict)\n",
    "        test_set_to_probability_dataframe_dict[test_set_suffix] = probability_dataframe\n",
    "    return test_set_to_probability_dataframe_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "singletask_scar_model_path = \"../trained_models/single-task-scar/resnet34d_384_LVEF50\"\n",
    "single_task_scar_clinical_model_path = \"../trained_models/single-task-scar-clinical/resnet34d_384_LVEF50_birnn_dim512\"\n",
    "singletask_lvef_model_path = \"../trained_models/single-task-lvef/resnet34d_384_LVEF50\"\n",
    "singletask_lvef_clinical_model_path = \"../trained_models/single-task-lvef-clinical/resnet34d_384_LVEF50_birnn_dim512\"\n",
    "\n",
    "multitask_old_format_model_path = \"../trained_models/multi-task-old-format/resnet34d_384_LVEF50\"\n",
    "multitask_transferred_model_path = \"../trained_models/multi-task-transferred/resnet34d_384_LVEF50\"\n",
    "multitask_model_path = \"../trained_models/multi-task/resnet34d_384_LVEF50\"\n",
    "multitask_clinical_model_path = \"../trained_models/multi-task-clinical/resnet34d_384_LVEF50_birnn_dim512/\"\n",
    "\n",
    "xgboost_scar_model_path = \"../trained_models/xgboost-clinical/scar_model\"\n",
    "xgboost_lvef_model_path = \"../trained_models/xgboost-clinical/lvef_model\"\n",
    "\n",
    "probability_save_dir = \"../resources/prediction_probabilities\"\n",
    "os.makedirs(probability_save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [\"scar\", \"lvef\"]\n",
    "for task in tasks:\n",
    "    model_name_to_dir_map = {\n",
    "        \"multi-task-old-format\": multitask_old_format_model_path,\n",
    "        \"multi-task-transferred\": multitask_transferred_model_path,\n",
    "        \"single-task\": singletask_scar_model_path if task == \"scar\" else singletask_lvef_model_path,\n",
    "        \"multi-task\": multitask_model_path,\n",
    "        \"single-task-clinical\": singletask_scar_clinical_model_path if task == \"scar\" else singletask_lvef_clinical_model_path,\n",
    "        \"multi-task-clinical\": multitask_clinical_model_path,\n",
    "        \"xgboost-clinical\": xgboost_scar_model_path if task == \"scar\" else xgboost_lvef_model_path,\n",
    "    }\n",
    "\n",
    "    test_set_to_probability_dataframe_dict = get_probabilities(\n",
    "        model_name_to_dir_map=model_name_to_dir_map,\n",
    "        test_set_suffix_list=TEST_SET_SAVE_SUFFIX_LIST,\n",
    "        task=task,\n",
    "    )\n",
    "\n",
    "    for test_set_suffix, probability_dataframe in test_set_to_probability_dataframe_dict.items():\n",
    "        probability_save_path = op.join(probability_save_dir, f\"{task}_probabilities_{test_set_suffix}.csv\")\n",
    "        probability_dataframe.to_csv(probability_save_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BBB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "import pandas as pd\n",
    "from mtecg.evaluation import calculate_metrics\n",
    "\n",
    "def recompute_metrics_with_bbb(\n",
    "    model_dir: str,\n",
    "    test_set_name: str,\n",
    "    csv_with_qrs_path: str,\n",
    "    image_dir: str,\n",
    "    imputer_dir: str\n",
    "    ):\n",
    "    def is_bbb(qrs_duration):\n",
    "        return qrs_duration > 120\n",
    "\n",
    "    prediction_result_path = op.join(model_dir, f\"prediction_{test_set_name}.csv\")\n",
    "    prediction_df = pd.read_csv(prediction_result_path)\n",
    "\n",
    "    # Load the ECG dataframe\n",
    "    test_df_with_qrs = load_ecg_dataframe(\n",
    "        csv_with_qrs_path,\n",
    "        image_dir,\n",
    "        imputer_dir=imputer_dir,\n",
    "        do_split=False\n",
    "    )\n",
    "\n",
    "    # Join prediction_df with test_df_with_qrs on filename.\n",
    "    prediction_df.rename(columns={\"filename\": \"file_name\"}, inplace=True)\n",
    "    test_df_with_qrs.rename(columns={\"filename\": \"file_name\"}, inplace=True)\n",
    "    prediction_df_with_qrs = prediction_df.merge(test_df_with_qrs, on=\"file_name\", how=\"left\")\n",
    "\n",
    "    # Apply the is_bbb function to create a new column 'is_bbb'\n",
    "    prediction_df_with_qrs[\"is_bbb\"] = prediction_df_with_qrs[\"qrs_duration\"].apply(is_bbb)\n",
    "\n",
    "    # Split data into two DataFrames based on 'is_bbb' column\n",
    "    prediction_df_with_bbb = prediction_df_with_qrs[prediction_df_with_qrs[\"is_bbb\"]].reset_index(drop=True)\n",
    "    prediction_df_without_bbb = prediction_df_with_qrs[~prediction_df_with_qrs[\"is_bbb\"]].reset_index(drop=True)\n",
    "\n",
    "    task_list = []\n",
    "    for column in prediction_df.columns:\n",
    "        if column.__contains__(\"scar\"):\n",
    "            task_list.append(\"scar\")\n",
    "        elif column.__contains__(\"lvef\"):\n",
    "            task_list.append(\"lvef\")\n",
    "    task_list = list(set(task_list))\n",
    "    # Rearrange the task_list to always have 'scar' as the first element if it exists.\n",
    "    if \"scar\" in task_list:\n",
    "        task_list.remove(\"scar\")\n",
    "        task_list.insert(0, \"scar\")\n",
    "\n",
    "    # Calculate metrics for both DataFrames\n",
    "    metrics_df_with_bbb = calculate_metrics(prediction_df_with_bbb, tasks=task_list)\n",
    "    metrics_df_without_bbb = calculate_metrics(prediction_df_without_bbb, tasks=task_list)\n",
    "    \n",
    "    metrics_df_with_bbb = metrics_df_with_bbb[[]]\n",
    "\n",
    "    return metrics_df_with_bbb, metrics_df_without_bbb\n",
    "\n",
    "# # Usage example:\n",
    "# model_dir = multitask_model_path\n",
    "# test_set_name = \"new_test\"\n",
    "# csv_with_qrs_path = f\"../datasets/{test_set_name}_with_qrs_duration.csv\"\n",
    "# # image_dir = old_image_dir\n",
    "# image_dir = new_image_dir\n",
    "# imputer_dir = multitask_clinical_model_path\n",
    "\n",
    "# metrics_df_with_bbb, metrics_df_without_bbb = recompute_metrics_with_bbb(model_dir, test_set_name, csv_with_qrs_path, image_dir, imputer_dir)\n",
    "\n",
    "test_set_name_list = [\"old_test\", \"old_test_sensitivity\", \"new_test\", \"new_test_sensitivity\"]\n",
    "model_dir_list = [\n",
    "    singletask_scar_model_path,\n",
    "    singletask_scar_clinical_model_path,\n",
    "    singletask_lvef_model_path,\n",
    "    singletask_lvef_clinical_model_path,\n",
    "    multitask_old_format_model_path,\n",
    "    multitask_transferred_model_path,\n",
    "    multitask_model_path,\n",
    "    multitask_clinical_model_path,\n",
    "]\n",
    "\n",
    "for model_dir in model_dir_list:\n",
    "    for test_set_name in test_set_name_list:\n",
    "        print(f\"model_dir: {model_dir}, test_set_name: {test_set_name}\")\n",
    "\n",
    "        if test_set_name.__contains__(\"old_test\"):\n",
    "            test_set_type = \"old_test\"\n",
    "        else:\n",
    "            test_set_type = \"new_test\"\n",
    "\n",
    "\n",
    "        csv_with_qrs_path = f\"../datasets/{test_set_type}_with_qrs_duration.csv\"\n",
    "        image_dir = old_image_dir if test_set_type == \"old_test\" else new_image_dir\n",
    "        imputer_dir = multitask_clinical_model_path\n",
    "\n",
    "        metrics_df_with_bbb, metrics_df_without_bbb = recompute_metrics_with_bbb(model_dir, test_set_name, csv_with_qrs_path, image_dir, imputer_dir)\n",
    "        metrics_df_with_bbb.to_csv(f\"{model_dir}/metrics_{test_set_name}_with_bbb.csv\", index=True)\n",
    "        metrics_df_without_bbb.to_csv(f\"{model_dir}/metrics_{test_set_name}_without_bbb.csv\", index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "109b74d06ceef90c267188b655f808679842e5df9d924ed4ca45afc3047e2ff5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
