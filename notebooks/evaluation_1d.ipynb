{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Preparations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\ecg\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Anaconda3\\envs\\ecg\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "c:\\Anaconda3\\envs\\ecg\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "c:\\Anaconda3\\envs\\ecg\\lib\\site-packages\\pkg_resources\\__init__.py:123: PkgResourcesDeprecationWarning: llow is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path as op\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from mtecg.classifier import ECGClassifier\n",
    "from mtecg.evaluation import evaluate_from_dataframe_1d\n",
    "from mtecg.utils import load_ecg_dataframe_1d\n",
    "import mtecg.constants as constants\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "round_probabilities = False\n",
    "\n",
    "singletask_scar_model_path = \"../trained_models/1d/single-task-scar/resnet34d_200\"\n",
    "singletask_lvef_model_path = \"../trained_models/1d/single-task-lvef/resnet34d_200_LVEF50\"\n",
    "multitask_model_path = \"../trained_models/1d/multi-task/resnet34d_200_LVEF50\"\n",
    "multitask_old_format_model_path = \"../trained_models/1d/multi-task-old-format/resnet34d_200_LVEF50\"\n",
    "multitask_transferred_model_path = \"../trained_models/1d/multi-task-transferred/resnet34d_200_LVEF50\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "singletask_scar_classifier = ECGClassifier(\n",
    "    singletask_scar_model_path,\n",
    "    model_class=\"single-task-1d\",\n",
    "    device=device,\n",
    "    task=\"scar\",\n",
    "    round_probabilities=round_probabilities,\n",
    ")\n",
    "singletask_lvef_classifier = ECGClassifier(\n",
    "    singletask_lvef_model_path,\n",
    "    model_class=\"single-task-1d\",\n",
    "    device=device,\n",
    "    task=\"lvef\",\n",
    "    round_probabilities=round_probabilities,\n",
    ")\n",
    "\n",
    "multitask_classifier = ECGClassifier(\n",
    "    multitask_model_path, model_class=\"multi-task-1d\", device=device, round_probabilities=round_probabilities\n",
    ")\n",
    "multitask_old_format_classifier = ECGClassifier(\n",
    "    multitask_old_format_model_path, model_class=\"multi-task-1d\", device=device, round_probabilities=round_probabilities\n",
    ")\n",
    "multitask_transferred_classifier = ECGClassifier(\n",
    "    multitask_transferred_model_path, model_class=\"multi-task-1d\", device=device, round_probabilities=round_probabilities\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_csv_path = \"../datasets/all_ECG_cleared_duplicate_may23_final_1d_labels_with_rpeaks.csv\"\n",
    "old_data_dir = \"../datasets/siriraj_data/ECG_MRI_1d_data_interp\"\n",
    "new_csv_path = \"../datasets/all_ECG_cleared_duplicate_may23_final_1d_labels_with_rpeaks.csv\"\n",
    "new_data_dir = \"../datasets/siriraj_data/ECG_MRI_1d_data_interp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old test set.\n",
    "old_test_df = load_ecg_dataframe_1d(\n",
    "    old_csv_path,\n",
    "    old_data_dir,\n",
    "    # imputer_dir=multitask_clinical_model_path,\n",
    "    do_split=True\n",
    ")\n",
    "old_test_df = old_test_df[old_test_df[\"split\"] == \"old_test\"].reset_index(drop=True)\n",
    "\n",
    "# New test set. No need to impute.\n",
    "new_test_df = load_ecg_dataframe_1d(\n",
    "    new_csv_path,\n",
    "    new_data_dir,\n",
    "    # imputer_dir=multitask_clinical_model_path,\n",
    "    do_split=True,\n",
    ")\n",
    "\n",
    "new_test_df = new_test_df[new_test_df[\"split\"] == \"new_test\"].reset_index(drop=True)\n",
    "\n",
    "# New test set with lvef_threshold= 40. No need to impute.\n",
    "sensitivity_new_test_df = load_ecg_dataframe_1d(\n",
    "    new_csv_path,\n",
    "    new_data_dir,\n",
    "    # imputer_dir=multitask_clinical_model_path,\n",
    "    do_split=False,\n",
    "    lvef_threshold=40,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56f8315a4ea14b789fba8d05c5740cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/895 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4dfab0d29dc4fc38c5c1f49c27175b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/895 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Before filter out: (895, 35)\n",
      "Shape After filter out: (895, 35)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9f36066e3994fe1bfcafb51c170935a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/895 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5baed7f5ad2447580ad0cc17c54ca80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/895 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape After filter out: (895, 35)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe5fb1e887a74f30a76dcc4c7a3d9d58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/895 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d325cfd546d6436196e5e7eb5238ca1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1264 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13142988f96a4c2d96f8a166e5c165b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1264 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Before filter out: (1264, 35)\n",
      "Shape After filter out: (1264, 35)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fccb3bca022847c0b4d63a846ec0fe76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1264 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd5d6f308f4544fbb4107e50075b8cac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1264 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape After filter out: (1263, 35)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ff033affd3e49d1a369bacf83cfa454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1263 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "from scipy.signal import resample\n",
    "import biosppy\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "ECG_FORMAT_TO_PAPER_PIXEL_SAMPLE_RATE = {\n",
    "    'old': 294,\n",
    "    'new': 280\n",
    "}\n",
    "\n",
    "def apply_rpeaks_normalization(lead_array_list, rpeak_index, ecg_format):\n",
    "    paper_pixel_sample_rate = ECG_FORMAT_TO_PAPER_PIXEL_SAMPLE_RATE[ecg_format]\n",
    "    start = max(0, int(rpeak_index - 0.5 * paper_pixel_sample_rate))\n",
    "    end = int(rpeak_index + 1.5 * paper_pixel_sample_rate)\n",
    "    return [lead[start:end] for lead in lead_array_list]\n",
    "\n",
    "def has_empty_leads(lead_array_list):\n",
    "    for lead in lead_array_list:\n",
    "        if len(lead) == 0:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def filter_out_empty_leads(dataframe):\n",
    "    print(f\"Shape Before filter out: {dataframe.shape}\")\n",
    "    dataframe['has_empty_leads'] = dataframe['lead_arrays'].progress_apply(lambda lead_list: has_empty_leads(lead_list))\n",
    "    dataframe = dataframe[dataframe['has_empty_leads'] != True].reset_index(drop=True)\n",
    "    print(f\"Shape After filter out: {dataframe.shape}\")\n",
    "    return dataframe\n",
    "\n",
    "def resample_leads(lead_array_list, sample_rate: int = 200):\n",
    "    for i, lead in enumerate(lead_array_list):\n",
    "        try:\n",
    "            lead_array_list[i] = resample(lead, sample_rate)\n",
    "        except:\n",
    "            print(f\"Error resampling lead {i}\")\n",
    "            print(lead)\n",
    "            print(len(lead))\n",
    "\n",
    "    return lead_array_list\n",
    "\n",
    "def process_dataframe(dataframe, sample_rate: int = 200):\n",
    "    dataframe['lead_arrays'] = dataframe[constants.path_column_name].progress_apply(lambda x: pickle.load(open(x, \"rb\")))\n",
    "    dataframe['has_empty_leads'] = dataframe['lead_arrays'].progress_apply(lambda lead_list: has_empty_leads(lead_list))\n",
    "    print(f\"Shape Before filter out: {dataframe.shape}\")\n",
    "    dataframe = dataframe[dataframe['has_empty_leads'] != True].reset_index(drop=True)\n",
    "    print(f\"Shape After filter out: {dataframe.shape}\")\n",
    "    dataframe['lead_arrays'] = dataframe.progress_apply(lambda row: apply_rpeaks_normalization(row['lead_arrays'], row['median_first_rpeak_index'], row['ecg_format']), axis=1)\n",
    "    dataframe['has_empty_leads'] = dataframe['lead_arrays'].progress_apply(lambda lead_list: has_empty_leads(lead_list))\n",
    "    dataframe = dataframe[dataframe['has_empty_leads'] != True].reset_index(drop=True)\n",
    "    print(f\"Shape After filter out: {dataframe.shape}\")\n",
    "    dataframe['lead_arrays'] = dataframe['lead_arrays'].progress_apply(lambda lead_list: resample_leads(lead_list, sample_rate))\n",
    "    return dataframe\n",
    "\n",
    "old_test_df = process_dataframe(old_test_df, 200)\n",
    "new_test_df = process_dataframe(new_test_df, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mtecg.datasets_1d import ECG1DDataset\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# dataset = ECG1DDataset(\n",
    "#     dataframe=old_test_df,\n",
    "#     label_column=\"scar_cad\",\n",
    "# )\n",
    "# dataloader = DataLoader(dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# dataloader_iter = iter(dataloader)\n",
    "\n",
    "# for i in range(1):\n",
    "#     data = next(dataloader_iter)\n",
    "#     print(data[0].shape)\n",
    "#     print(data[1].shape)\n",
    "\n",
    "\n",
    "# leads_batch = data[0]\n",
    "# batch_size = leads_batch.shape[0]\n",
    "# leads_batch = leads_batch.view(batch_size * 12, 1, -1)\n",
    "# leads_batch = leads_batch.unsqueeze(1)\n",
    "# leads_batch.shape\n",
    "\n",
    "\n",
    "# model = singletask_scar_classifier.model\n",
    "# lead_embeddings = model.model(leads_batch.to(device))\n",
    "\n",
    "# lead_embeddings = lead_embeddings.view(batch_size, 12, -1)\n",
    "# lead_embeddings.shape\n",
    "# torch.mean(lead_embeddings, dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For convenience.\n",
    "\n",
    "EVAL_DATA_MAP = {\n",
    "    \"old-test\": {\"data\": old_test_df, \"save_suffix\": \"old_test\",},\n",
    "    # \"old-test-sensitivity\": {\"data\": sensitivity_old_test_df, \"save_suffix\": \"old_test_sensitivity\",},\n",
    "    \"new-test\": {\"data\": new_test_df, \"save_suffix\": \"new_test\",},\n",
    "    # \"new-test-sensitivity\": {\"data\": sensitivity_new_test_df, \"save_suffix\": \"new_test_sensitivity\",},\n",
    "    # \"control-test\": {\"data\": control_test_df, \"save_suffix\": \"control_test\",},\n",
    "}\n",
    "\n",
    "TEST_SET_SAVE_SUFFIX_LIST = [param_dict[\"save_suffix\"] for param_dict in EVAL_DATA_MAP.values()]\n",
    "\n",
    "\n",
    "def evaluate_and_save(\n",
    "    classifier: ECGClassifier,\n",
    "    save_dir: str,\n",
    "    average: str = \"weighted\",\n",
    "    prediction_csv_name_pattern: str = \"prediction_{save_suffix}.csv\",\n",
    "    metric_csv_name_pattern: str = \"metrics_{save_suffix}.csv\",\n",
    "):\n",
    "    for test_set_name, param_dict in tqdm(EVAL_DATA_MAP.items()):\n",
    "        dataframe, save_suffix = param_dict[\"data\"], param_dict[\"save_suffix\"]\n",
    "        if \"control\" in test_set_name:\n",
    "            result_dataframe, metric_dataframe = evaluate_from_dataframe_1d(\n",
    "                dataframe,\n",
    "                classifier,\n",
    "                is_control_population=True,\n",
    "                average=average,\n",
    "                )\n",
    "        else:\n",
    "            result_dataframe, metric_dataframe = evaluate_from_dataframe_1d(dataframe, classifier)\n",
    "\n",
    "        result_save_path = op.join(save_dir, prediction_csv_name_pattern.format(save_suffix=save_suffix))\n",
    "        metric_save_path = op.join(save_dir, metric_csv_name_pattern.format(save_suffix=save_suffix))\n",
    "\n",
    "        result_dataframe.to_csv(result_save_path, index=False)\n",
    "        metric_dataframe.to_csv(metric_save_path, index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Evaluation**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Baseline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "def get_baseline_metrics(\n",
    "    save_dir: str = None,\n",
    "    label_column_name: str = \"scar_cad\",\n",
    "    metric_csv_name_pattern: str = \"{save_suffix}.csv\",\n",
    "    average: str = \"weighted\",\n",
    "    ):\n",
    "    baseline_metric_dict = {}\n",
    "    for test_set_name, param_dict in EVAL_DATA_MAP.items():\n",
    "        if \"control\" in test_set_name:\n",
    "            continue\n",
    "        dataframe = param_dict[\"data\"]\n",
    "        baseline_predictions = np.zeros(len(dataframe))\n",
    "\n",
    "        # get specificity from confusion matrix\n",
    "        tn, fp, fn, tp = confusion_matrix(dataframe[label_column_name], baseline_predictions).ravel()\n",
    "        specificity = tn / (tn+fp)\n",
    "        fpr = fp / (fp+tn)\n",
    "\n",
    "        baseline_metric_dict[test_set_name] = {\n",
    "            \"Accuracy\": accuracy_score(dataframe[label_column_name], baseline_predictions),\n",
    "            \"Sensitivity\": None,\n",
    "            \"Specificity\": specificity,\n",
    "            \"F1\": f1_score(dataframe[label_column_name], baseline_predictions, average=average),\n",
    "            \"AUC\": None,\n",
    "            \"FPR\": fpr,\n",
    "            \"FNR\": None,\n",
    "        }\n",
    "\n",
    "    if save_dir:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        for test_set_name, param_dict in EVAL_DATA_MAP.items():\n",
    "            if \"control\" in test_set_name:\n",
    "                continue\n",
    "            save_suffix = param_dict[\"save_suffix\"]\n",
    "            metric_save_path = op.join(save_dir, metric_csv_name_pattern.format(save_suffix=save_suffix))\n",
    "            pd.DataFrame(baseline_metric_dict[test_set_name], index=[0]).T.to_csv(metric_save_path, index=True)\n",
    "\n",
    "    return baseline_metric_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scar_baseline_metric_dict = get_baseline_metrics(\n",
    "    save_dir=op.join(\"../resources/statistics/1d/scar_baseline_metrics\"),\n",
    "    label_column_name=\"scar_cad\",\n",
    ")\n",
    "\n",
    "lvef_baseline_metric_dict = get_baseline_metrics(\n",
    "    save_dir=op.join(\"../resources/statistics/1d/lvef_baseline_metrics\"),\n",
    "    label_column_name=\"lvef\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **single-task**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4350863edddd4b6c899ac1330c2b0751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaab9c706b614b298869e281cbe3d615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40ee2ab661f44e00978bbb9ef175b22d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_and_save(singletask_scar_classifier, save_dir=singletask_scar_model_path)\n",
    "# evaluate_and_save(singletask_lvef_classifier, save_dir=singletask_lvef_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0    722\n",
       " 1    173\n",
       " Name: lvef_label, dtype: int64,\n",
       " 0    611\n",
       " 1    284\n",
       " Name: lvef_prediction, dtype: int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_df = pd.read_csv(op.join(singletask_lvef_model_path, \"prediction_old_test.csv\"))\n",
    "prediction_df.lvef_label.value_counts(), prediction_df.lvef_prediction.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0    1035\n",
       " 1     228\n",
       " Name: lvef_label, dtype: int64,\n",
       " 0    1018\n",
       " 1     245\n",
       " Name: lvef_prediction, dtype: int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_df = pd.read_csv(op.join(singletask_lvef_model_path, \"prediction_new_test.csv\"))\n",
    "prediction_df.lvef_label.value_counts(), prediction_df.lvef_prediction.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **single-task-clinical**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_and_save(singletask_scar_clinical_classifier, save_dir=singletask_scar_clinical_model_path)\n",
    "evaluate_and_save(singletask_lvef_clinical_classifier, save_dir=singletask_lvef_clinical_model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **multi-task**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71383f659ecf4a1ab229f8ec11778c7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ec2842b13fa4b91a5bd59289f34a4ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f568112122324d669e5123d0620624d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_and_save(multitask_classifier, save_dir=multitask_model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**multi-task-old-format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bea4d89cf814066a225ec726569594d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e861c6d7f1d049a2b620f00d62fb4910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "269d4aae41ed40d7a9702e5c1248d744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_and_save(multitask_old_format_classifier, save_dir=multitask_old_format_model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**multi-task-transferred**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7262d4013f0433cb21b6ac3ad7c462a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7504e54c5e2e43bd9b4d120798eb0152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90ff5f6fa17843ceb7689c38a1ecc12f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_and_save(multitask_transferred_classifier, save_dir=multitask_transferred_model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **multi-task-clinical**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwrite the EVAL_DATA_MAP to only evaluate the test sets with available clinical data.\n",
    "EVAL_DATA_MAP = {\n",
    "    \"old-test\": {\"data\": old_test_df, \"save_suffix\": \"old_test\",},\n",
    "    \"old-test-sensitivity\": {\"data\": sensitivity_old_test_df, \"save_suffix\": \"old_test_sensitivity\",},\n",
    "    \"new-test\": {\"data\": new_test_df, \"save_suffix\": \"new_test\",},\n",
    "    \"new-test-sensitivity\": {\"data\": sensitivity_new_test_df, \"save_suffix\": \"new_test_sensitivity\",},\n",
    "}\n",
    "\n",
    "evaluate_and_save(multitask_clinical_classifier, save_dir=multitask_clinical_model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[scar] Prevalence-specific Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def sample(dataframe, num_samples: int = 500, prevalence: float = 7.9):\n",
    "    \"\"\"\n",
    "    Sample a dataframe to a prevalence of 7.9% (the prevalence of scar in the dataset).\n",
    "    \"\"\"\n",
    "    positive_dataframe = dataframe[dataframe.scar_label == 1].reset_index(drop=True).copy()\n",
    "    negative_dataframe = dataframe[dataframe.scar_label == 0].reset_index(drop=True).copy()\n",
    "    \n",
    "    num_positive_samples = int(num_samples * prevalence / 100)\n",
    "    num_negative_samples = num_samples - num_positive_samples\n",
    "    \n",
    "    random_state = random.randint(0, 1000)\n",
    "    positive_sample_dataframe = positive_dataframe.sample(n=num_positive_samples, random_state = random_state).reset_index(drop=True)\n",
    "    negative_sample_dataframe = negative_dataframe.sample(n=num_negative_samples, random_state = random_state).reset_index(drop=True)\n",
    "    sampled_dataframe = pd.concat([positive_sample_dataframe, negative_sample_dataframe]).reset_index(drop=True)\n",
    "    return sampled_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "from mtecg.evaluation import calculate_metrics\n",
    "\n",
    "num_sample_per_iteration = 500\n",
    "# num_iteration_list = [20, 50, 200]\n",
    "num_iteration_list = [1000]\n",
    "\n",
    "\n",
    "multitask_prediction_df = pd.read_csv(op.join(multitask_model_path, \"prediction_new_test.csv\"))\n",
    "\n",
    "for num_iteration in tqdm(num_iteration_list):\n",
    "    scar_auc_list = []\n",
    "    scar_f1_list = []\n",
    "    for i in tqdm(range(num_iteration)):\n",
    "        sampled_test_df = sample(multitask_prediction_df, num_samples = num_sample_per_iteration)\n",
    "        metric_df = calculate_metrics(sampled_test_df, tasks=[\"scar\"])\n",
    "        # _, metric_df = evaluate_from_dataframe(sampled_test_df, multitask_classifier)\n",
    "        # _, metric_df = evaluate_from_dataframe(sampled_clinical_test_df, multitask_clinical_classifier)\n",
    "        scar_auc_list.append(metric_df.T[\"AUC\"][0])\n",
    "        scar_f1_list.append(metric_df.T[\"F1\"][0])\n",
    "\n",
    "    # Create 95% confidence interval for population mean scar auc.\n",
    "    auc_confidence_interval_tuple = st.t.interval(\n",
    "        alpha=0.95,\n",
    "        df=len(scar_auc_list)-1,\n",
    "        loc=np.mean(scar_auc_list),\n",
    "        scale=st.sem(scar_auc_list)\n",
    "        )\n",
    "    \n",
    "    f1_confidence_interval_tuple = st.t.interval(\n",
    "        alpha=0.95,\n",
    "        df=len(scar_f1_list)-1,\n",
    "        loc=np.mean(scar_f1_list),\n",
    "        scale=st.sem(scar_f1_list)\n",
    "        )\n",
    "\n",
    "    auc_summary_df = pd.DataFrame(\n",
    "        { \n",
    "            \"mean\": [np.mean(scar_auc_list)],\n",
    "            \"std\": [np.std(scar_auc_list)],\n",
    "            \"lower_bound_ci\": [auc_confidence_interval_tuple[0]],\n",
    "            \"upper_bound_ci\": [auc_confidence_interval_tuple[1]],\n",
    "            }\n",
    "    )\\\n",
    "        .round(4)\n",
    "\n",
    "    f1_summary_df = pd.DataFrame(\n",
    "        { \n",
    "            \"mean\": [np.mean(scar_f1_list)],\n",
    "            \"std\": [np.std(scar_f1_list)],\n",
    "            \"lower_bound_ci\": [f1_confidence_interval_tuple[0]],\n",
    "            \"upper_bound_ci\": [f1_confidence_interval_tuple[1]],\n",
    "            }\n",
    "    )\\\n",
    "        .round(4)\n",
    "\n",
    "    auc_summary_df.to_csv(op.join(multitask_model_path, f\"prevalence_specific_auc_{num_sample_per_iteration}_{num_iteration}.csv\"), index=False)\n",
    "    f1_summary_df.to_csv(op.join(multitask_model_path, f\"prevalence_specific_f1_{num_sample_per_iteration}_{num_iteration}.csv\"), index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Save Prediction Probabilities on Each Test Set as a single file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Dict, List\n",
    "\n",
    "# A function to read the predictions from the save csv file in each model folder.\n",
    "# The probability columns of each task are then concatenated into a single dataframe.\n",
    "# The probability columns are in the format f\"{task}_probability\".\n",
    "\n",
    "def get_probabilities(\n",
    "    model_name_to_dir_map: Dict[str, str],\n",
    "    test_set_suffix_list: List[str],\n",
    "    probability_column_name_pattern: str = \"{task}_probability\",\n",
    "    prediction_csv_name_pattern: str = \"prediction_{test_set_suffix}.csv\",\n",
    "    task: str = \"scar\",\n",
    ") -> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Get the probabilities of the given task from the predictions of the models.\n",
    "    \"\"\"\n",
    "    probability_column_name = probability_column_name_pattern.format(task=task)\n",
    "\n",
    "    test_set_to_probability_dataframe_dict = {}\n",
    "    for test_set_suffix in test_set_suffix_list:\n",
    "        if \"control\" in test_set_suffix:\n",
    "            continue\n",
    "        model_name_to_probabilities_dict = {}\n",
    "        for model_name, model_dir in model_name_to_dir_map.items():\n",
    "            filename = prediction_csv_name_pattern.format(test_set_suffix=test_set_suffix)\n",
    "            prediction_path = op.join(model_dir, filename)\n",
    "            prediction_dataframe = pd.read_csv(prediction_path)\n",
    "            if \"true_label\" not in model_name_to_probabilities_dict.keys():\n",
    "                model_name_to_probabilities_dict[\"true_label\"] = prediction_dataframe[f\"{task}_label\"]\n",
    "            model_name_to_probabilities_dict[model_name] = prediction_dataframe[probability_column_name]\n",
    "        probability_dataframe = pd.DataFrame(model_name_to_probabilities_dict)\n",
    "        test_set_to_probability_dataframe_dict[test_set_suffix] = probability_dataframe\n",
    "    return test_set_to_probability_dataframe_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "singletask_scar_model_path = \"../trained_models/1d/single-task-scar/resnet34d_200\"\n",
    "singletask_lvef_model_path = \"../trained_models/1d/single-task-lvef/resnet34d_200_LVEF50\"\n",
    "multitask_model_path = \"../trained_models/1d/multi-task/resnet34d_200_LVEF50\"\n",
    "multitask_old_format_model_path = \"../trained_models/1d/multi-task-old-format/resnet34d_200_LVEF50\"\n",
    "multitask_transferred_model_path = \"../trained_models/1d/multi-task-transferred/resnet34d_200_LVEF50\"\n",
    "\n",
    "probability_save_dir = \"../resources/prediction_probabilities_1d\"\n",
    "os.makedirs(probability_save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [\"scar\", \"lvef\"]\n",
    "for task in tasks:\n",
    "    model_name_to_dir_map = {\n",
    "        \"multi-task-old-format\": multitask_old_format_model_path,\n",
    "        \"multi-task-transferred\": multitask_transferred_model_path,\n",
    "        \"single-task\": singletask_scar_model_path if task == \"scar\" else singletask_lvef_model_path,\n",
    "        \"multi-task\": multitask_model_path,\n",
    "    }\n",
    "\n",
    "    test_set_to_probability_dataframe_dict = get_probabilities(\n",
    "        model_name_to_dir_map=model_name_to_dir_map,\n",
    "        test_set_suffix_list=TEST_SET_SAVE_SUFFIX_LIST,\n",
    "        task=task,\n",
    "    )\n",
    "\n",
    "    for test_set_suffix, probability_dataframe in test_set_to_probability_dataframe_dict.items():\n",
    "        probability_save_path = op.join(probability_save_dir, f\"{task}_probabilities_{test_set_suffix}.csv\")\n",
    "        probability_dataframe.to_csv(probability_save_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "109b74d06ceef90c267188b655f808679842e5df9d924ed4ca45afc3047e2ff5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
