{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\ecg\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Anaconda3\\envs\\ecg\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "c:\\Anaconda3\\envs\\ecg\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "c:\\Anaconda3\\envs\\ecg\\lib\\site-packages\\pkg_resources\\__init__.py:123: PkgResourcesDeprecationWarning: llow is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from mtecg.utils import load_ecg_dataframe\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dev_csv_path = \"../../ECG_EF_Clin_train_dev_new.csv\"\n",
    "# new_test_csv_path = \"../../ECG_EF_Clin_test_new_nocut_noimpute.csv\"\n",
    "\n",
    "train_dev_csv_path = \"../datasets/all_ECG_cleared_duplicate_may23_final.csv\"\n",
    "new_test_csv_path = \"../datasets/all_ECG_cleared_duplicate_may23_final.csv\"\n",
    "imputer_dir = \"../trained_models/multi-task-clinical/resnet34d_384_LVEF50_birnn_dim512\"\n",
    "\n",
    "train_dev_image_dir = \"../datasets/siriraj_data/ECG_MRI_images_new/\"\n",
    "new_test_image_dir = \"../datasets/siriraj_data/ECG_MRI_test_images_new/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old test set.\n",
    "train_dev_df = load_ecg_dataframe(\n",
    "    train_dev_csv_path,\n",
    "    train_dev_image_dir,\n",
    "    # imputer_dir=imputer_dir,\n",
    "    do_split=True,\n",
    "    return_lvef_40_column=True,\n",
    ")\n",
    "# New test set. No need to impute.\n",
    "new_test_df = load_ecg_dataframe(\n",
    "    new_test_csv_path,\n",
    "    new_test_image_dir,\n",
    "    # imputer_dir=imputer_dir,\n",
    "    do_split=False,\n",
    "    return_lvef_40_column=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scar_type_excel_path = \"../../AI_ECG_CAD_scar_type_221227.xlsx\"\n",
    "scar_type_df = pd.read_excel(scar_type_excel_path)\n",
    "\n",
    "# select cols from scar\n",
    "scar_type_df = scar_type_df[[\"File_Name\",\"Month\",\"Subendocardial_scar\",\"Transmural_scar\",\"Subendocardial_scar_or_Transmural_scar\"]]\n",
    "# Lowercase column names for consistency.\n",
    "scar_type_df.columns = map(str.lower, scar_type_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge scar type onto train dev & test\n",
    "train_dev_df = pd.merge(train_dev_df, scar_type_df, on=[\"file_name\", \"month\"], how='left')\n",
    "new_test_df = pd.merge(new_test_df, scar_type_df, on=[\"file_name\", \"month\"], how='left')\n",
    "\n",
    "# remove 2 corrupted PDF\n",
    "# 2010/10/2010_401658221.pdf\n",
    "# 2016/6/2016_527006041.pdf\n",
    "train_dev_df = train_dev_df.drop(train_dev_df[train_dev_df['file_name'].isin([\"2010_401658221\", \"2016_527006041\"])].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = train_dev_df[train_dev_df[\"split\"].isin([\"old_train\", \"new_train\"]) == 1].reset_index(drop=True)\n",
    "# dev_df = train_dev_df[train_dev_df[\"split\"].isin([\"old_valid\", \"new_valid\"]) == 1].reset_index(drop=True)\n",
    "# old_test_df = train_dev_df[train_dev_df[\"split\"] == \"old_test\"].reset_index(drop=True)\n",
    "\n",
    "old_train_df = train_dev_df[train_dev_df[\"split\"] == \"old_train\"].reset_index(drop=True)\n",
    "old_valid_df = train_dev_df[train_dev_df[\"split\"] == \"old_valid\"].reset_index(drop=True)\n",
    "old_test_df = train_dev_df[train_dev_df[\"split\"] == \"old_test\"].reset_index(drop=True)\n",
    "new_train_df = train_dev_df[train_dev_df[\"split\"] == \"new_train\"].reset_index(drop=True)\n",
    "new_valid_df = train_dev_df[train_dev_df[\"split\"] == \"new_valid\"].reset_index(drop=True)\n",
    "\n",
    "population_to_df_map_dict = {\n",
    "    \"Training (Old format)\": old_train_df,\n",
    "    \"Training (New format)\": new_train_df,\n",
    "    \"Development (Old format)\": old_valid_df,\n",
    "    \"Development (New format)\": new_valid_df,\n",
    "    \"Test (Old format)\": old_test_df,\n",
    "    \"Test (New format)\": new_test_df,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9393, 34), (2500, 34), (895, 34), (1264, 34))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, dev_df.shape, old_test_df.shape, new_test_df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Get stats for the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_and_prevalence(dataframe: pd.DataFrame, col: str, inverse: bool = False) -> tuple:\n",
    "    total_samples = dataframe.shape[0]\n",
    "    target_columns = [col]\n",
    "    target_class_index = 0 if inverse else 1\n",
    "\n",
    "    target_dataframe = dataframe[target_columns].copy()\n",
    "    n = target_dataframe.value_counts()[target_class_index]\n",
    "    prevalence = n/total_samples * 100\n",
    "\n",
    "    n = round(n, 3)\n",
    "    prevalence = round(prevalence, 3)\n",
    "    return n, prevalence\n",
    "\n",
    "# result in % prevalence \n",
    "def get_baseline_stats(\n",
    "    dataframe: pd.DataFrame,\n",
    "    lvef_col: str = \"lvef\",\n",
    "    lvef_40_col: str = \"lvef_40\",\n",
    "    scar_col: str = \"scar_cad\",\n",
    "    age_col: str = \"age\",\n",
    "    female_gender_col: str = \"female_gender\",\n",
    "    smoke_col: str = \"smoke\",\n",
    "    dm_col: str = \"dm\",\n",
    "    ht_col: str = \"ht\",\n",
    "    dlp_col: str = \"dlp\",\n",
    "    SubS_col: str = \"subendocardial_scar\",\n",
    "    TranS_col: str = \"transmural_scar\",\n",
    "    ):\n",
    "    n_samples = dataframe.shape[0]\n",
    "    # Default values.\n",
    "    lvef_prevalence = 0\n",
    "    scar_prevalence = 0\n",
    "    # Calculate prevalence of LVEF and scar if the columns are present.\n",
    "    if lvef_col in dataframe.columns:\n",
    "        lvef_n, lvef_prevalence = get_n_and_prevalence(dataframe, lvef_col)\n",
    "    if lvef_40_col in dataframe.columns:\n",
    "        lvef_40_n, lvef_40_prevalence = get_n_and_prevalence(dataframe, lvef_40_col)\n",
    "    if scar_col in dataframe.columns:\n",
    "        scar_n, scar_prevalence = get_n_and_prevalence(dataframe, scar_col)\n",
    "\n",
    "    # Calculate baseline statistics.\n",
    "    mean_age = dataframe[age_col].mean()\n",
    "    std_age = dataframe[age_col].std()\n",
    "\n",
    "    # Scale to back to original values.\n",
    "    mean_age = round(mean_age * 100, 3) \n",
    "    std_age = round(std_age * 100, 3)\n",
    "\n",
    "    male_n, male_percent = get_n_and_prevalence(dataframe, female_gender_col, inverse=True)\n",
    "    smoke_n, smoke_percent = get_n_and_prevalence(dataframe, smoke_col)\n",
    "    ht_n, ht_percent = get_n_and_prevalence(dataframe, ht_col)\n",
    "    dm_n, dm_percent = get_n_and_prevalence(dataframe, dm_col)\n",
    "    dlp_n, dlp_percent = get_n_and_prevalence(dataframe, dlp_col)\n",
    "    SubEn_n, SubEn_prevalence = get_n_and_prevalence(dataframe, SubS_col)\n",
    "    TranMu_n, TranMu_prevalence = get_n_and_prevalence(dataframe, TranS_col)\n",
    "\n",
    "    SubEn_and_TranMu_n = dataframe[(dataframe[SubS_col] == 1) & (dataframe[TranS_col] == 1)].shape[0]\n",
    "    SubEn_and_TranMu_prevalence = SubEn_and_TranMu_n/n_samples * 100\n",
    "    SubEn_and_TranMu_prevalence = round(SubEn_and_TranMu_prevalence, 3)\n",
    "\n",
    "    baseline_stat_tuple_dict = {\n",
    "        \"age\": [f\"{mean_age} +/- {std_age}\"],\n",
    "        \"male\": [f\"{male_n} ({male_percent})\"],\n",
    "        \"smoke\": [f\"{smoke_n} ({smoke_percent})\"],\n",
    "        \"ht\": [f\"{ht_n} ({ht_percent})\"],\n",
    "        \"dm\": [f\"{dm_n} ({dm_percent})\"],\n",
    "        \"dlp\": [f\"{dlp_n} ({dlp_percent})\"],\n",
    "        \"scar\": [f\"{scar_n} ({scar_prevalence})\"],\n",
    "        \"lvef\": [f\"{lvef_n} ({lvef_prevalence})\"],\n",
    "        \"lvef_40\": [f\"{lvef_40_n} ({lvef_40_prevalence})\"],\n",
    "        \"SubEn\": [f\"{SubEn_n} ({SubEn_prevalence})\"],\n",
    "        \"TranMu\": [f\"{TranMu_n} ({TranMu_prevalence})\"],\n",
    "        \"SubEn_and_TranMu\": [f\"{SubEn_and_TranMu_n} ({SubEn_and_TranMu_prevalence})\"],\n",
    "    }\n",
    "\n",
    "    baseline_stat_dataframe =  pd.DataFrame(baseline_stat_tuple_dict)\n",
    "    return baseline_stat_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistic_dataframe_list = []\n",
    "for population_name, dataframe in population_to_df_map_dict.items():\n",
    "    baseline_stat_dataframe = get_baseline_stats(\n",
    "        dataframe,\n",
    "        SubS_col = \"subs\",\n",
    "        TranS_col = \"trans\",\n",
    "        )\n",
    "    baseline_stat_dataframe.index = [population_name]\n",
    "    statistic_dataframe_list.append(baseline_stat_dataframe)\n",
    "\n",
    "all_population_df = pd.concat(population_to_df_map_dict.values(), axis=0)\n",
    "population_stat_df = get_baseline_stats(\n",
    "    all_population_df,\n",
    "    SubS_col = \"subs\",\n",
    "    TranS_col = \"trans\",\n",
    "    )\n",
    "population_stat_df.index = [\"population\"]\n",
    "statistic_dataframe_list.append(population_stat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(statistic_dataframe_list, axis=0)#.to_csv(\"../resources/statistics/population_statistics_split.csv\", index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "109b74d06ceef90c267188b655f808679842e5df9d924ed4ca45afc3047e2ff5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
