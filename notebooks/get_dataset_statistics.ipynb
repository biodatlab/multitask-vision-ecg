{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\ecg\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Anaconda3\\envs\\ecg\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "c:\\Anaconda3\\envs\\ecg\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "c:\\Anaconda3\\envs\\ecg\\lib\\site-packages\\pkg_resources\\__init__.py:123: PkgResourcesDeprecationWarning: llow is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from mtecg.utils import load_ecg_dataframe\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dev_csv_path = \"../../ECG_EF_Clin_train_dev_new.csv\"\n",
    "new_test_csv_path = \"../../ECG_EF_Clin_test_new_nocut_noimpute.csv\"\n",
    "imputer_dir = \"../trained_models/multi-task-clinical/resnet34d_384_LVEF50_birnn_dim512\"\n",
    "\n",
    "train_dev_image_dir = \"../../ecg/ecg-cnn-local/siriraj_data/ECG_MRI_images_new/\"\n",
    "new_test_image_dir = \"../../ecg/ecg-cnn-local/siriraj_data/ECG_MRI_test_images_new/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old test set.\n",
    "train_dev_df = load_ecg_dataframe(\n",
    "    train_dev_csv_path,\n",
    "    train_dev_image_dir,\n",
    "    imputer_dir=imputer_dir,\n",
    "    do_split=True,\n",
    "    return_lvef_40_column=True,\n",
    ")\n",
    "# New test set. No need to impute.\n",
    "new_test_df = load_ecg_dataframe(\n",
    "    new_test_csv_path,\n",
    "    new_test_image_dir,\n",
    "    # imputer_dir=imputer_dir,\n",
    "    do_split=False,\n",
    "    return_lvef_40_column=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scar_type_excel_path = \"../../AI_ECG_CAD_scar_type_221227.xlsx\"\n",
    "scar_type_df = pd.read_excel(scar_type_excel_path)\n",
    "\n",
    "# select cols from scar\n",
    "scar_type_df = scar_type_df[[\"File_Name\",\"Month\",\"Subendocardial_scar\",\"Transmural_scar\",\"Subendocardial_scar_or_Transmural_scar\"]]\n",
    "# Lowercase column names for consistency.\n",
    "scar_type_df.columns = map(str.lower, scar_type_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge scar type onto train dev & test\n",
    "train_dev_df = pd.merge(train_dev_df, scar_type_df, on=[\"file_name\", \"month\"], how='left')\n",
    "new_test_df = pd.merge(new_test_df, scar_type_df, on=[\"file_name\", \"month\"], how='left')\n",
    "\n",
    "# remove 2 corrupted PDF\n",
    "# 2010/10/2010_401658221.pdf\n",
    "# 2016/6/2016_527006041.pdf\n",
    "train_dev_df = train_dev_df.drop(train_dev_df[train_dev_df['file_name'].isin([\"2010_401658221\", \"2016_527006041\"])].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_dev_df[train_dev_df[\"split\"].isin([\"old_train\", \"new_train\"]) == 1].reset_index(drop=True)\n",
    "dev_df = train_dev_df[train_dev_df[\"split\"].isin([\"old_valid\", \"new_valid\"]) == 1].reset_index(drop=True)\n",
    "old_test_df = train_dev_df[train_dev_df[\"split\"] == \"old_test\"].reset_index(drop=True)\n",
    "\n",
    "population_to_df_map_dict = {\n",
    "    \"train\": train_df,\n",
    "    \"dev\": dev_df,\n",
    "    \"old_test\": old_test_df,\n",
    "    \"new_test\": new_test_df,\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Get stats for the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_and_prevalence(dataframe: pd.DataFrame, col: str, inverse: bool = False) -> tuple:\n",
    "    total_samples = dataframe.shape[0]\n",
    "    target_columns = [col]\n",
    "    target_class_index = 0 if inverse else 1\n",
    "\n",
    "    target_dataframe = dataframe[target_columns].copy()\n",
    "    n = target_dataframe.value_counts()[target_class_index]\n",
    "    prevalence = n/total_samples * 100\n",
    "\n",
    "    n = round(n, 3)\n",
    "    prevalence = round(prevalence, 3)\n",
    "    return n, prevalence\n",
    "\n",
    "# result in % prevalence \n",
    "def get_baseline_stats(\n",
    "    dataframe: pd.DataFrame,\n",
    "    lvef_col: str = \"lvef\",\n",
    "    lvef_40_col: str = \"lvef_40\",\n",
    "    scar_col: str = \"scar_cad\",\n",
    "    age_col: str = \"age\",\n",
    "    female_gender_col: str = \"female_gender\",\n",
    "    smoke_col: str = \"smoke\",\n",
    "    dm_col: str = \"dm\",\n",
    "    ht_col: str = \"ht\",\n",
    "    dlp_col: str = \"dlp\",\n",
    "    SubS_col: str = \"subendocardial_scar\",\n",
    "    TranS_col: str = \"transmural_scar\",\n",
    "    ):\n",
    "    n_samples = dataframe.shape[0]\n",
    "    # Default values.\n",
    "    lvef_prevalence = 0\n",
    "    scar_prevalence = 0\n",
    "    # Calculate prevalence of LVEF and scar if the columns are present.\n",
    "    if lvef_col in dataframe.columns:\n",
    "        lvef_n, lvef_prevalence = get_n_and_prevalence(dataframe, lvef_col)\n",
    "    if lvef_40_col in dataframe.columns:\n",
    "        lvef_40_n, lvef_40_prevalence = get_n_and_prevalence(dataframe, lvef_40_col)\n",
    "    if scar_col in dataframe.columns:\n",
    "        scar_n, scar_prevalence = get_n_and_prevalence(dataframe, scar_col)\n",
    "\n",
    "    # Calculate baseline statistics.\n",
    "    mean_age = dataframe[age_col].mean()\n",
    "    std_age = dataframe[age_col].std()\n",
    "\n",
    "    # Scale to back to original values.\n",
    "    mean_age = round(mean_age * 100, 3) \n",
    "    std_age = round(std_age * 100, 3)\n",
    "\n",
    "    male_n, male_percent = get_n_and_prevalence(dataframe, female_gender_col, inverse=True)\n",
    "    smoke_n, smoke_percent = get_n_and_prevalence(dataframe, smoke_col)\n",
    "    ht_n, ht_percent = get_n_and_prevalence(dataframe, ht_col)\n",
    "    dm_n, dm_percent = get_n_and_prevalence(dataframe, dm_col)\n",
    "    dlp_n, dlp_percent = get_n_and_prevalence(dataframe, dlp_col)\n",
    "    SubEn_n, SubEn_prevalence = get_n_and_prevalence(dataframe, SubS_col)\n",
    "    TranMu_n, TranMu_prevalence = get_n_and_prevalence(dataframe, TranS_col)\n",
    "\n",
    "    SubEn_or_TranMu_n = dataframe[(dataframe[SubS_col] == 1) & (dataframe[TranS_col] == 1)].shape[0]\n",
    "    SubEn_or_TranMu_prevalence = SubEn_or_TranMu_n/n_samples * 100\n",
    "    SubEn_or_TranMu_prevalence = round(SubEn_or_TranMu_prevalence, 3)\n",
    "\n",
    "    baseline_stat_tuple_dict = {\n",
    "        \"age\": [f\"{mean_age} +/- {std_age}\"],\n",
    "        \"male\": [f\"{male_n} ({male_percent})\"],\n",
    "        \"smoke\": [f\"{smoke_n} ({smoke_percent})\"],\n",
    "        \"ht\": [f\"{ht_n} ({ht_percent})\"],\n",
    "        \"dm\": [f\"{dm_n} ({dm_percent})\"],\n",
    "        \"dlp\": [f\"{dlp_n} ({dlp_percent})\"],\n",
    "        \"scar\": [f\"{scar_n} ({scar_prevalence})\"],\n",
    "        \"lvef\": [f\"{lvef_n} ({lvef_prevalence})\"],\n",
    "        \"lvef_40\": [f\"{lvef_40_n} ({lvef_40_prevalence})\"],\n",
    "        \"SubEn\": [f\"{SubEn_n} ({SubEn_prevalence})\"],\n",
    "        \"TranMu\": [f\"{TranMu_n} ({TranMu_prevalence})\"],\n",
    "        \"SubEn_or_TranMu\": [f\"{SubEn_or_TranMu_n} ({SubEn_or_TranMu_prevalence})\"],\n",
    "    }\n",
    "\n",
    "    baseline_stat_dataframe =  pd.DataFrame(baseline_stat_tuple_dict)\n",
    "    return baseline_stat_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistic_dataframe_list = []\n",
    "for population_name, dataframe in population_to_df_map_dict.items():\n",
    "    baseline_stat_dataframe = get_baseline_stats(dataframe)\n",
    "    baseline_stat_dataframe.index = [population_name]\n",
    "    statistic_dataframe_list.append(baseline_stat_dataframe)\n",
    "\n",
    "all_population_df = pd.concat(population_to_df_map_dict.values(), axis=0)\n",
    "population_stat_df = get_baseline_stats(all_population_df)\n",
    "population_stat_df.index = [\"population\"]\n",
    "statistic_dataframe_list.append(population_stat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(statistic_dataframe_list, axis=0).to_csv(\"../resources/statistics/population_statistics.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>male</th>\n",
       "      <th>smoke</th>\n",
       "      <th>ht</th>\n",
       "      <th>dm</th>\n",
       "      <th>dlp</th>\n",
       "      <th>scar</th>\n",
       "      <th>lvef</th>\n",
       "      <th>lvef_40</th>\n",
       "      <th>SubEn</th>\n",
       "      <th>TranMu</th>\n",
       "      <th>SubEn_or_TranMu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>73.205 +/- 13.825</td>\n",
       "      <td>4822 (51.336)</td>\n",
       "      <td>1584 (16.864)</td>\n",
       "      <td>7063 (75.194)</td>\n",
       "      <td>3457 (36.804)</td>\n",
       "      <td>6655 (70.851)</td>\n",
       "      <td>2655 (28.266)</td>\n",
       "      <td>1803 (19.195)</td>\n",
       "      <td>1197 (12.744)</td>\n",
       "      <td>1978 (21.058)</td>\n",
       "      <td>1693 (18.024)</td>\n",
       "      <td>1016 (10.817)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dev</th>\n",
       "      <td>70.593 +/- 13.932</td>\n",
       "      <td>1396 (48.055)</td>\n",
       "      <td>280 (9.639)</td>\n",
       "      <td>1734 (59.69)</td>\n",
       "      <td>853 (29.363)</td>\n",
       "      <td>1590 (54.733)</td>\n",
       "      <td>720 (24.785)</td>\n",
       "      <td>489 (16.833)</td>\n",
       "      <td>316 (10.878)</td>\n",
       "      <td>492 (16.936)</td>\n",
       "      <td>460 (15.835)</td>\n",
       "      <td>232 (7.986)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>old_test</th>\n",
       "      <td>71.592 +/- 14.013</td>\n",
       "      <td>531 (50.813)</td>\n",
       "      <td>157 (15.024)</td>\n",
       "      <td>815 (77.99)</td>\n",
       "      <td>401 (38.373)</td>\n",
       "      <td>766 (73.301)</td>\n",
       "      <td>277 (26.507)</td>\n",
       "      <td>192 (18.373)</td>\n",
       "      <td>122 (11.675)</td>\n",
       "      <td>188 (17.99)</td>\n",
       "      <td>189 (18.086)</td>\n",
       "      <td>100 (9.569)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new_test</th>\n",
       "      <td>69.961 +/- 14.494</td>\n",
       "      <td>751 (50.641)</td>\n",
       "      <td>47 (3.169)</td>\n",
       "      <td>641 (43.223)</td>\n",
       "      <td>331 (22.32)</td>\n",
       "      <td>528 (35.604)</td>\n",
       "      <td>401 (27.04)</td>\n",
       "      <td>263 (17.734)</td>\n",
       "      <td>171 (11.531)</td>\n",
       "      <td>241 (16.251)</td>\n",
       "      <td>294 (19.825)</td>\n",
       "      <td>134 (9.036)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>population</th>\n",
       "      <td>72.255 +/- 13.986</td>\n",
       "      <td>7500 (50.587)</td>\n",
       "      <td>2068 (13.948)</td>\n",
       "      <td>10253 (69.156)</td>\n",
       "      <td>5042 (34.008)</td>\n",
       "      <td>9539 (64.34)</td>\n",
       "      <td>4053 (27.337)</td>\n",
       "      <td>2747 (18.528)</td>\n",
       "      <td>1806 (12.181)</td>\n",
       "      <td>2899 (19.553)</td>\n",
       "      <td>2636 (17.78)</td>\n",
       "      <td>1482 (9.996)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          age           male          smoke              ht  \\\n",
       "train       73.205 +/- 13.825  4822 (51.336)  1584 (16.864)   7063 (75.194)   \n",
       "dev         70.593 +/- 13.932  1396 (48.055)    280 (9.639)    1734 (59.69)   \n",
       "old_test    71.592 +/- 14.013   531 (50.813)   157 (15.024)     815 (77.99)   \n",
       "new_test    69.961 +/- 14.494   751 (50.641)     47 (3.169)    641 (43.223)   \n",
       "population  72.255 +/- 13.986  7500 (50.587)  2068 (13.948)  10253 (69.156)   \n",
       "\n",
       "                       dm            dlp           scar           lvef  \\\n",
       "train       3457 (36.804)  6655 (70.851)  2655 (28.266)  1803 (19.195)   \n",
       "dev          853 (29.363)  1590 (54.733)   720 (24.785)   489 (16.833)   \n",
       "old_test     401 (38.373)   766 (73.301)   277 (26.507)   192 (18.373)   \n",
       "new_test      331 (22.32)   528 (35.604)    401 (27.04)   263 (17.734)   \n",
       "population  5042 (34.008)   9539 (64.34)  4053 (27.337)  2747 (18.528)   \n",
       "\n",
       "                  lvef_40          SubEn         TranMu SubEn_or_TranMu  \n",
       "train       1197 (12.744)  1978 (21.058)  1693 (18.024)   1016 (10.817)  \n",
       "dev          316 (10.878)   492 (16.936)   460 (15.835)     232 (7.986)  \n",
       "old_test     122 (11.675)    188 (17.99)   189 (18.086)     100 (9.569)  \n",
       "new_test     171 (11.531)   241 (16.251)   294 (19.825)     134 (9.036)  \n",
       "population  1806 (12.181)  2899 (19.553)   2636 (17.78)    1482 (9.996)  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat(statistic_dataframe_list, axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "109b74d06ceef90c267188b655f808679842e5df9d924ed4ca45afc3047e2ff5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
